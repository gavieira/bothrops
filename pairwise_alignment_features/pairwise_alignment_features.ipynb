{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO, AlignIO\n",
    "import os, itertools, subprocess\n",
    "import pandas\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "genbank = [\"BDIP_ARG.gb\", \"BPUB_RS.gb\", \"Bothrops_jararaca-NC_030760.1.gb\", \"Bothrops_pubescens-NC_039648.1.gb\", \"Bothrops_diporus-NC_039649.1.gb\"]\n",
    "print(len(genbank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BDIP_ARG.gb \n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'BDIP_ARG.gb'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-f2f84565cd64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mfeature_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenbank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-52-f2f84565cd64>\u001b[0m in \u001b[0;36mfeature_parser\u001b[0;34m(genbank)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mrecovered_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mgb_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeqIO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"genbank\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0morganism\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgb_handle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"organism\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mall_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0morganism\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/Bio/SeqIO/__init__.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(handle, format, alphabet)\u001b[0m\n\u001b[1;32m    702\u001b[0m     \u001b[0mto\u001b[0m \u001b[0mread\u001b[0m \u001b[0mmultiple\u001b[0m \u001b[0mrecords\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m     \"\"\"\n\u001b[0;32m--> 704\u001b[0;31m     \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malphabet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m         \u001b[0mrecord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/Bio/SeqIO/__init__.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(handle, format, alphabet)\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0miterator_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0malphabet\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m             \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miterator_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/Bio/SeqIO/InsdcIO.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, source)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \"\"\"\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"GenBank\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/Bio/SeqIO/Interfaces.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, source, alphabet, mode, fmt)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malphabet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malphabet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_close_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# not a path, assume we received a stream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'BDIP_ARG.gb'"
     ]
    }
   ],
   "source": [
    "fasta_set = set()\n",
    "\n",
    "def feature_parser(genbank):\n",
    "    all_features = dict()\n",
    "    for gb in genbank:\n",
    "        recovered_features = list()\n",
    "        print(gb, \"\\n\")\n",
    "        gb_handle = SeqIO.read(gb, \"genbank\")\n",
    "        organism = gb_handle.annotations.get(\"organism\").replace(\" \", \"_\")\n",
    "        all_features[organism] = {}\n",
    "        for feature in gb_handle.features:\n",
    "            gene_name = ''\n",
    "            seq = feature.location.extract(gb_handle.seq)\n",
    "            if feature.type == \"CDS\":\n",
    "                #continue\n",
    "                #print(feature.qualifiers.get('gene')[0])\n",
    "                gene_name = feature.qualifiers.get('gene')[0]\n",
    "            elif feature.type == \"tRNA\":\n",
    "                #continue\n",
    "                #print(feature.qualifiers.get('product')[0])\n",
    "                gene_name = feature.qualifiers.get('product')[0]\n",
    "            elif feature.type == \"rRNA\":\n",
    "                if feature.qualifiers.get('product')[0].startswith('small'):\n",
    "                    feature.qualifiers['product'] = ['12S ribosomal RNA']\n",
    "                if feature.qualifiers.get('product')[0].startswith('large'):\n",
    "                    feature.qualifiers['product'] = ['16S ribosomal RNA']\n",
    "                #print(feature.qualifiers.get('product')[0].split(\" \")[0],'\\n',seq)\n",
    "                gene_name = feature.qualifiers.get('product')[0].split(\" \")[0]\n",
    "            if gene_name != '':\n",
    "                if gene_name in recovered_features:\n",
    "                    gene_name = \"{}2\".format(gene_name)\n",
    "                recovered_features.append(gene_name)\n",
    "                feature_dict = {gene_name : seq}\n",
    "                all_features[organism].update(feature_dict)\n",
    "                \n",
    "                #with open(fasta_name, \"a+\") as fasta:\n",
    "                    #fasta_set.add(fasta_name)\n",
    "                    #fasta.write(\">{}_{}\\n{}\\n\".format(organism,gene_name,seq))\n",
    "                #print(header)\n",
    "                #print(recovered_features)\n",
    "                #print(len(fasta_set))\n",
    "                #print(fasta_set)\n",
    "                #print('>{} {}'.format(header, organism))\n",
    "        #print(\"----------------------------\\n\")\n",
    "    return(all_features)\n",
    "\n",
    "feature_dict = feature_parser(genbank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gene in feature_dict.values():\n",
    "        print(gene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_feature_from_dict(feature):\n",
    "    for gene in feature_dict.values():\n",
    "        if feature in gene.keys():\n",
    "            del gene[feature]\n",
    "        \n",
    "def count_features(feature_dict):\n",
    "    counter = dict()\n",
    "    for gene in feature_dict.values():\n",
    "        for i in gene.keys():\n",
    "            if i not in counter:\n",
    "                counter[i] = 1\n",
    "            else:\n",
    "                counter[i] += 1\n",
    "    return(counter)\n",
    "\n",
    "def remove_missing_features(feature_dict):                \n",
    "    counter = count_features(feature_dict)\n",
    "    print(counter)\n",
    "    for key, value in counter.items():\n",
    "        if value < len(feature_dict):\n",
    "            print(key)\n",
    "            remove_feature_from_dict(key)\n",
    "    return(feature_dict)\n",
    "\n",
    "feature_dict = remove_missing_features(feature_dict)\n",
    "\n",
    "    \n",
    "#new_feature_dict = remove_missing_features(feature_dict)\n",
    "        \n",
    "#print(len(feature_dict))\n",
    "#print(count_features)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in feature_dict.keys():\n",
    "    print(feature_dict[i].keys(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder(sp1, sp2):\n",
    "    folder_name = \"{}_vs_{}\".format(sp1, sp2)\n",
    "    os.mkdir(folder_name)\n",
    "    os.chdir(folder_name)\n",
    "    print(\"Running clustal alingment for {}\".format(folder_name))\n",
    "\n",
    "def run_clustalo(sp1, sp2, feature_dict):\n",
    "    sp1_feat = feature_dict[sp1]\n",
    "    sp2_feat = feature_dict[sp2]\n",
    "    clustal_list = list()\n",
    "    for i in sp1_feat.keys():\n",
    "        clustal_output = \"{}.fa\".format(i)\n",
    "        clustal_error = \"{}.err\".format(i)\n",
    "        with open(\"tempfile\", \"w+\") as tempfile, open(clustal_output, \"w+\") as clustal_out, open(clustal_error, \"w+\") as clustal_err:\n",
    "            tempfile.write(\">{}\\n{}\\n>{}\\n{}\".format(sp1, sp1_feat[i], sp2, sp2_feat[i]))\n",
    "            tempfile.close()\n",
    "            #time.sleep(2)\n",
    "            alignment = subprocess.Popen([\"clustalo\", \"-i\", \"tempfile\"], stdout=clustal_out, stderr=clustal_err)\n",
    "            alignment.wait()\n",
    "            os.remove(\"tempfile\")\n",
    "        if os.path.getsize(clustal_output) != 0 and os.path.getsize(clustal_error) == 0:\n",
    "            os.remove(clustal_error)\n",
    "            clustal_list.append(clustal_output)\n",
    "        else:\n",
    "            print(\"There was a problem with the {}_vs{} alignment. Check the file {}\".format(sp1, sp2, clustal_error))\n",
    "    return(clustal_list)\n",
    "\n",
    "def ambiguous_sites_as_gaps(clustal_list):\n",
    "    print(\"Replacing ambiguous sites with gap characters...\")\n",
    "    data = str()\n",
    "    for aln in clustal_list:\n",
    "        data = str()\n",
    "        data2 = str()\n",
    "        print(aln)\n",
    "        alignment = AlignIO.read(aln, \"fasta\")\n",
    "        for index in [0, 1]: ##Index of each fasta record in the alignment - in this case, 0 and 1 (pairwise)\n",
    "            valid_nucs = [\"A\", \"T\", \"G\", \"C\"]\n",
    "            unambiguous_seq = str()\n",
    "            for nuc in alignment[index].seq:\n",
    "                if nuc in valid_nucs:\n",
    "                    unambiguous_seq += nuc\n",
    "                else:\n",
    "                    unambiguous_seq += \"-\"\n",
    "            data += \">{} : {}\\n\".format(alignment[index].id, len(unambiguous_seq))\n",
    "            data2 += \">{}\\n{}\\n\".format(alignment[index].id, unambiguous_seq)\n",
    "        print(data)\n",
    "        with open(aln, \"w\") as unambiguous_aln:\n",
    "            unambiguous_aln.write(data2)\n",
    "\n",
    "def concat_alignment(clustal_list):\n",
    "    ##Besides the merging, this function needs to generate a clustal list with all the absolute paths to\n",
    "    ##the final alignment files, to be used as input to the \"alignment_parser function\"\n",
    "    header1 = set()\n",
    "    header2 = set()\n",
    "    seq1 = \"\"\n",
    "    seq2 = \"\"\n",
    "    size = 0\n",
    "    number_of_features = 0\n",
    "    concat_filename = \"\"\n",
    "    for num, i in enumerate(clustal_list, 1):\n",
    "        aln = AlignIO.read(i, \"fasta\")\n",
    "        header1.add(aln[0].id)\n",
    "        header2.add(aln[1].id)\n",
    "        seq1 += aln[0].seq\n",
    "        seq2 += aln[1].seq\n",
    "        size += aln.get_alignment_length()\n",
    "        number_of_features = num\n",
    "    if len(header1) == len(header2) == 1 and size == len(seq1) == len(seq2):\n",
    "        header1 = list(header1)[0]\n",
    "        header2 = list(header2)[0]\n",
    "        concat_filename = \"{}_vs_{}_concat.fa\".format(header1, header2)\n",
    "        print(header1, header2)\n",
    "        print(len(seq1), len(seq2))\n",
    "        print(\"Features: {}, Alignment_size: {}\".format(number_of_features, size))\n",
    "        with open(concat_filename, \"w\") as concat:\n",
    "            concat.write(\">{}\\n{}\\n>{}\\n{}\\n\".format(header1, seq1, header2, seq2))\n",
    "    else:\n",
    "        print(\"There was a problem in the merging of alignments in {}\".format(concat_filename))\n",
    "        return(None)\n",
    "    return(os.path.abspath(concat_filename))\n",
    "    \n",
    "def pairwise_permutations(feature_dict):\n",
    "    '''Receives a dictionary of multiple sequence entries (e.g. {header1 : seq1, header2 : seq2, ... headerN : seqN}) and returns a list of all possible pairwise permutations between the dictionary's keys.'''\n",
    "    permutations = list()\n",
    "    global concat_list\n",
    "    concat_list = list()\n",
    "    for sp1, sp2 in itertools.combinations(feature_dict.keys(), 2):\n",
    "        create_folder(sp1, sp2)\n",
    "        clustal_list = run_clustalo(sp1, sp2, feature_dict)\n",
    "        ambiguous_sites_as_gaps(clustal_list)\n",
    "        concat_list.append(concat_alignment(clustal_list))\n",
    "        os.chdir(\"..\")        \n",
    "        \n",
    "pairwise_permutations(feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ambiguous_sites_as_gaps(clustal_list):\n",
    "    print(\"Replacing ambiguous sites with gap characters...\")\n",
    "    data = str()\n",
    "    for aln in clustal_list:\n",
    "        data = str()\n",
    "        data2 = str()\n",
    "        print(aln)\n",
    "        alignment = AlignIO.read(aln, \"fasta\")\n",
    "        for index in [0, 1]: ##Index of each fasta record in the alignment - in this case, 0 and 1 (pairwi)\n",
    "            valid_nucs = [\"A\", \"T\", \"G\", \"C\"]\n",
    "            unambiguous_seq = str()\n",
    "            for nuc in alignment[index].seq:\n",
    "                if nuc in valid_nucs:\n",
    "                    unambiguous_seq += nuc\n",
    "                else:\n",
    "                    unambiguous_seq += \"-\"\n",
    "            data += \">{} : {}\\n\".format(alignment[index].id, len(unambiguous_seq))\n",
    "            data2 += \">{}\\n{}\\n\".format(alignment[index].id, unambiguous_seq)\n",
    "        print(data)\n",
    "        with open(aln, \"w\") as unambiguous_aln:\n",
    "            unambiguous_aln.write(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unambiguous_check_length(clustal_list):\n",
    "    for file in clustal_list:\n",
    "        length = set()\n",
    "        aln = AlignIO.read(file, \"fasta\")\n",
    "        for i in range(len(aln)):\n",
    "            length.update([len(aln[i])])\n",
    "        if len(length) == 1:\n",
    "            print(\"The sequences in {} have the same length - {} - Conversion to unambiguous SUCCESSFUL!\".format(file, length))\n",
    "        else:\n",
    "            print(\"The sequences in {} have different lengths - {} - Conversion to unambiguous FAILED!\".format(file, length))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustal_parser(clustal_list):\n",
    "    fields = [\"Query MT\", \"Subject MT\", \"Total Alignment Sites\", \"Gapped Sites\", \"Valid Sites\", \"Identical Sites\", \"Variant Sites\", \"Identity Percentage\"]\n",
    "    excel_output_dict = {k : list() for k in fields}\n",
    "    for file in clustal_list: ##First, we need to get each information from the alignment and assign it to a variable\n",
    "        aln = AlignIO.read(file, \"fasta\")\n",
    "        query_seq = aln[0].id\n",
    "        subject_seq = aln[1].id\n",
    "        total_aln_sites = aln.get_alignment_length()\n",
    "        gap_sites = list()\n",
    "        gap_sites_count = 0\n",
    "        identical_sites = list()\n",
    "        identical_sites_count = 0\n",
    "        variant_sites = list()\n",
    "        variant_sites_count = 0\n",
    "        collumns = dict()\n",
    "        for i in range(1, total_aln_sites + 1):\n",
    "            collumns.update({i : aln[:, i-1]})\n",
    "        for k, v in collumns.items(): ##Get number of gapped sites\n",
    "            if \"-\" in v:\n",
    "                gap_sites_count += 1\n",
    "                gap_sites.append([k, v])\n",
    "                #print(k, v)\n",
    "                continue\n",
    "            nucs_in_collumn = set()\n",
    "            for i in [0,1]: ##Get number of identical and variable sites for pairwise alignment\n",
    "                nucs_in_collumn.update([v[i]])\n",
    "            if len(nucs_in_collumn) == 1:\n",
    "                identical_sites_count += 1\n",
    "                identical_sites.append([k, v])\n",
    "            if len(nucs_in_collumn) > 1:\n",
    "                variant_sites_count += 1\n",
    "                variant_sites.append([k, v])\n",
    "                #print(k, v)        \n",
    "        valid_sites = total_aln_sites - gap_sites_count\n",
    "        identity_percentage = round((identical_sites_count / valid_sites) * 100, 2)\n",
    "        \n",
    "        \n",
    "        ##After getting the variables, the script generates a file for each pairwise alignment, with a summary of the alignment's data\n",
    "        \n",
    "        parser_output = \"{}.clustalparser\".format(file[:-3])        \n",
    "        with open(parser_output, \"w+\") as cluparser:\n",
    "            cluparser.write(\"Query sequence: {} - A {} \\t T {} \\t G {} \\t C {} \\t gaps {}\\n\".format(query_seq, aln[0].seq.count(\"A\"), aln[0].seq.count(\"T\"), aln[0].seq.count(\"G\"), aln[0].seq.count(\"C\"), aln[0].seq.count(\"-\")))\n",
    "            cluparser.write(\"Subject sequence: {} - A {} \\t T {} \\t G {} \\t C {} \\t gaps {}\\n\".format(subject_seq, aln[1].seq.count(\"A\"), aln[1].seq.count(\"T\"), aln[1].seq.count(\"G\"), aln[1].seq.count(\"C\"), aln[1].seq.count(\"-\")))\n",
    "            cluparser.write(\"Total alignment size: {}\\n\".format(total_aln_sites))\n",
    "            cluparser.write(\"Gapped columns: {}\\n\".format(gap_sites_count))\n",
    "            cluparser.write(\"Valid columns (Total - Gapped): {}\\n\".format(valid_sites))\n",
    "            cluparser.write(\"Identical columns: {}\\n\".format(identical_sites_count))\n",
    "            cluparser.write(\"Variant columns: {}\\n\".format(variant_sites_count))\n",
    "            cluparser.write(\"Identity percentage - ((Identical collumns / Valid collumns) * 100): {} %\\n\".format(identity_percentage))\n",
    "            cluparser.write(\"VARIANT SITES:\\n\")\n",
    "            cluparser.write(\"Number\\tAlignment_site\\t{}_nt\\t{}_nt\\n\".format(query_seq, subject_seq))\n",
    "            [cluparser.write(\"{}\\t{}\\t{}\\t{}\\n\".format(i, v[0], v[1][0], v[1][1])) for i, v in enumerate(variant_sites, 1)]\n",
    "            cluparser.write(\"GAP SITES:\\n\")\n",
    "            cluparser.write(\"Number\\tAlignment_site\\t{}_nt\\t{}_nt\\n\".format(query_seq, subject_seq))\n",
    "            [cluparser.write(\"{}\\t{}\\t{}\\t{}\\n\".format(i, v[0], v[1][0], v[1][1])) for i, v in enumerate(gap_sites, 1)]\n",
    "            cluparser.write(\"IDENTICAL SITES:\\n\")\n",
    "            cluparser.write(\"Number\\tAlignment_site\\t{}_nt\\t{}_nt\\n\".format(query_seq, subject_seq))\n",
    "            [cluparser.write(\"{}\\t{}\\t{}\\t{}\\n\".format(i, v[0], v[1][0], v[1][1])) for i, v in enumerate(identical_sites, 1)]\n",
    "\n",
    "        ##Lastly, the script will save all data to a dictionary and return it. This dict will be used to create a pandas dataframe and write a excel spreadsheet\n",
    "            \n",
    "        excel_output_dict.get(\"Query MT\").append(query_seq)\n",
    "        excel_output_dict.get(\"Subject MT\").append(subject_seq)\n",
    "        excel_output_dict.get(\"Total Alignment Sites\").append(total_aln_sites)\n",
    "        excel_output_dict.get(\"Gapped Sites\").append(gap_sites_count)\n",
    "        excel_output_dict.get(\"Valid Sites\").append(valid_sites)\n",
    "        excel_output_dict.get(\"Identical Sites\").append(identical_sites_count)\n",
    "        excel_output_dict.get(\"Variant Sites\").append(variant_sites_count)\n",
    "        excel_output_dict.get(\"Identity Percentage\").append(identity_percentage)\n",
    "            \n",
    "        #if (gap_sites_count + identical_sites_count + variant_sites_count) == total_aln_sites:\n",
    "         #   print(\"The sum is correct\")\n",
    "        #else:\n",
    "        #    print(\"There was a problem with the parsing\")\n",
    "        #print(aln[1].seq.count(\"-\"))\n",
    "        \n",
    "    return(excel_output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_excel_aln_summary(excel_output_dict):\n",
    "    aln_summary = pandas.DataFrame(excel_output_dict)\n",
    "    aln_summary.to_excel(\"Pairwise_alignment_summary.xlsx\", index=False, sheet_name=\"Aln_summary\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel = clustal_parser(concat_list)\n",
    "generate_excel_aln_summary(excel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What to do from here:\n",
    "\n",
    "- [x] Create the pairwise comparison list using the function from the other script\n",
    "- [x] For each pairwise comparison, create a folder\n",
    "- [x] Inside each folder, there will be an fasta for each gene pair\n",
    "- [x] Each fasta will be used for an alignment\n",
    "- [ ] Lastly, the sequece of all alignments will be merged into a single supermatrix (could be in any order, maybe the synteny order?)\n",
    "- [ ] This supermatrix will be used as input for the clustal_parser function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
